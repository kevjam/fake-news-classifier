{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# For Saving/Loading Files\n",
    "from utils.storage import load_data,load_tokenizers\n",
    "\n",
    "# -------------- Modelling Packages --------------\n",
    "# For modeling\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Embedding, GRU, Bidirectional\n",
    "from keras.layers import Input, Reshape, SpatialDropout1D, Dense, Flatten\n",
    "from keras.layers import Concatenate\n",
    "from keras import optimizers\n",
    "\n",
    "# Callback Functions\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# For Timestamping Models\n",
    "import time\n",
    "\n",
    "# For balancing class weights\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# -------------- General Packages --------------\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "\n",
    "# Saving/Loading\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './models/'\n",
    "SPLIT_DATA_DIR = './split_data/'\n",
    "LOG_DIR = 'logs'\n",
    "TOKENIZER_DIR = './tokenizers/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset + Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_X_train,EN_X_test,ZH_X_train,ZH_X_test,y_train,y_test = load_data(SPLIT_DATA_DIR)\n",
    "t_EN,t_ZH = load_tokenizers(TOKENIZER_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(os.path.dirname(MODEL_DIR), exist_ok=True)\n",
    "\n",
    "# -------------- Tokenizer Values --------------\n",
    "EN_SENTENCE_SIZE = int(EN_X_train.shape[1]/2)\n",
    "ZH_SENTENCE_SIZE = int(ZH_X_train.shape[1]/2)\n",
    "en_vocab_size = len(t_EN.word_index) + 1\n",
    "zh_vocab_size = len(t_ZH.word_index) + 1\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n",
    "\n",
    "# -------------- TUNABLE HYPERPARAMETERS --------------\n",
    "EMBED_SIZES = [ 50 ]\n",
    "GRU_SIZES = [ 100 ]\n",
    "GRU_LAYER_SIZES = [ 1 ]\n",
    "\n",
    "# DROPOUT_SIZES = [(0.5,0)]\n",
    "# DROPOUT_SIZES = [(0,0),(0,0.5),(0.5,0),(0.5,0.5),\n",
    "#                  (0.25,0.25),(0.25,0.5),(0.5,0.25),(0.25,0),\n",
    "#                  (0,0.25),(0.75,0.75),(0.75,0.5),(0.5,0.75),\n",
    "#                  (0.75,0),(0,0.75),(0.75,0.25),(0.25,0.75)]\n",
    "\n",
    "DROPOUT_SIZES = [(0,0.75),(0.75,0.25),(0.25,0.75)]\n",
    "\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "optimizer = optimizers.adam()\n",
    "#optimizer = optimizers.sgd(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "metrics = ['accuracy']\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 1024\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BiGRU-50E-1x100G-(0, 0.75)Dropout-1574833242.4031265.hdf5\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 192223 samples, validate on 64087 samples\n",
      "Epoch 1/200\n",
      "  2048/192223 [..............................] - ETA: 8:13 - loss: 1.0551 - accuracy: 0.5420 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.224635). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55296/192223 [=======>......................] - ETA: 42s - loss: 0.7137 - accuracy: 0.6881"
     ]
    }
   ],
   "source": [
    "for EMBED_SIZE in EMBED_SIZES:\n",
    "    for DROPOUT_SIZE in DROPOUT_SIZES:\n",
    "        for GRU_SIZE in GRU_SIZES:\n",
    "            for GRU_LAYERS in GRU_LAYER_SIZES:\n",
    "                # -------------- MODEL NAMING --------------\n",
    "                NAME = 'BiGRU-{}E-{}x{}G-{}Dropout-{}.hdf5'.format(EMBED_SIZE,\n",
    "                                                                         GRU_LAYERS,GRU_SIZE,\n",
    "                                                                         DROPOUT_SIZE,\n",
    "                                                                         time.time())\n",
    "                print('Creating {}'.format(NAME))\n",
    "                MODEL_LOG_DIR = os.path.join(LOG_DIR,NAME)\n",
    "\n",
    "                # -------------- Callbacks --------------\n",
    "                # access tensorboard from the command line: tensorboard --logdir=logs/\n",
    "                tensorboard = TensorBoard(log_dir=MODEL_LOG_DIR) \n",
    "                checkpointer = ModelCheckpoint(MODEL_DIR+NAME, \n",
    "                                               monitor='val_accuracy', \n",
    "                                               verbose=1, \n",
    "                                               save_best_only=True, \n",
    "                                               mode='auto')\n",
    "                earlystop = EarlyStopping(monitor='val_loss', patience=PATIENCE)\n",
    "\n",
    "                callbacks=[tensorboard,checkpointer,earlystop]\n",
    "\n",
    "                # -------------- EN MODEL CREATION --------------\n",
    "                EN_INPUT = Input(shape=(EN_SENTENCE_SIZE*2,))\n",
    "                EN_MODEL = Reshape((-1,2,EN_SENTENCE_SIZE))(EN_INPUT)\n",
    "                EN_MODEL = Embedding(en_vocab_size,\n",
    "                                EMBED_SIZE,\n",
    "                                input_shape=(2,EN_SENTENCE_SIZE),\n",
    "                                trainable=True)(EN_MODEL)\n",
    "                EN_MODEL = Reshape((2,EN_SENTENCE_SIZE*EMBED_SIZE,))(EN_MODEL)\n",
    "                \n",
    "                if DROPOUT_SIZE[0] > 0: EN_MODEL = SpatialDropout1D(DROPOUT_SIZE[0])(EN_MODEL)\n",
    "                for layer in range(GRU_LAYERS-1):\n",
    "                    EN_MODEL = Bidirectional(GRU(GRU_SIZE,\n",
    "                                                  return_sequences=True, \n",
    "                                                  recurrent_dropout=DROPOUT_SIZE[1]))(EN_MODEL)\n",
    "                EN_MODEL = Bidirectional(GRU(GRU_SIZE,\n",
    "                                              return_sequences=True,\n",
    "                                              recurrent_dropout=DROPOUT_SIZE[1]))(EN_MODEL)\n",
    "    \n",
    "                # -------------- ZH MODEL CREATION --------------\n",
    "                ZH_INPUT = Input(shape=(ZH_SENTENCE_SIZE*2,))\n",
    "                ZH_MODEL = Reshape((-1,2,ZH_SENTENCE_SIZE))(ZH_INPUT)\n",
    "                ZH_MODEL = Embedding(zh_vocab_size,\n",
    "                                EMBED_SIZE,\n",
    "                                input_shape=(2,ZH_SENTENCE_SIZE),\n",
    "                                trainable=True)(ZH_MODEL)\n",
    "                ZH_MODEL = Reshape((2,ZH_SENTENCE_SIZE*EMBED_SIZE,))(ZH_MODEL)\n",
    "                \n",
    "                if DROPOUT_SIZE[0] > 0: ZH_MODEL = SpatialDropout1D(DROPOUT_SIZE[0])(ZH_MODEL)\n",
    "                for layer in range(GRU_LAYERS-1):\n",
    "                    ZH_MODEL = Bidirectional(GRU(GRU_SIZE,\n",
    "                                                  return_sequences=True, \n",
    "                                                  recurrent_dropout=DROPOUT_SIZE[1]))(ZH_MODEL)\n",
    "                ZH_MODEL = Bidirectional(GRU(GRU_SIZE,\n",
    "                                              return_sequences=True,\n",
    "                                              recurrent_dropout=DROPOUT_SIZE[1]))(ZH_MODEL)\n",
    "                    \n",
    "                # -------------- MERGE MODEL --------------\n",
    "                merged = Concatenate(1)([EN_MODEL,ZH_MODEL])\n",
    "                merged = GRU(GRU_SIZE,\n",
    "                              return_sequences=True,\n",
    "                              recurrent_dropout=DROPOUT_SIZE[1])(merged)\n",
    "                merged = Flatten()(merged)\n",
    "                merged = Dense(3, activation='softmax')(merged)\n",
    "\n",
    "                model = Model(inputs=[EN_INPUT,ZH_INPUT], outputs=merged)\n",
    "                model.compile(optimizer=optimizer, loss=loss,metrics=metrics)\n",
    "\n",
    "                # -------------- Training the model --------------\n",
    "                model.fit([EN_X_train,ZH_X_train], y_train,\n",
    "                          epochs=epochs,\n",
    "                          batch_size=batch_size,\n",
    "                          validation_data=([EN_X_test,ZH_X_test], y_test),\n",
    "                          callbacks=callbacks,\n",
    "                          class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
