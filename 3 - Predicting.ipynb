{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nreading from a CSV file\\n- https://www.youtube.com/watch?v=eEIr70i8vbs\\n\\nwriting to a CSV file\\n- https://www.youtube.com/watch?v=hmYdzvmcTD8\\n- https://stackoverflow.com/questions/34864695/saving-prediction-results-to-csv\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ********************** REFERENCES *****************************\n",
    "\n",
    "\"\"\"\n",
    "reading from a CSV file\n",
    "- https://www.youtube.com/watch?v=eEIr70i8vbs\n",
    "\n",
    "writing to a CSV file\n",
    "- https://www.youtube.com/watch?v=hmYdzvmcTD8\n",
    "- https://stackoverflow.com/questions/34864695/saving-prediction-results-to-csv\n",
    "\"\"\"\n",
    "\n",
    "# ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "//anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "//anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "//anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "//anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "//anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "//anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# ************************ IMPORTS ******************************\n",
    "\n",
    "# -------------- Modelling Packages --------------\n",
    "# For modeling\n",
    "from keras.models import Model\n",
    "from keras.layers import Concatenate, Input, Dense, Embedding\n",
    "from keras.layers import LSTM, Bidirectional, SpatialDropout1D\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import load_model\n",
    "\n",
    "# Callback Functions\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# For Timestamping Models\n",
    "import time\n",
    "\n",
    "# -------------- Preprocessing Packages --------------\n",
    "# For tokenizing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# -------------- General Packages --------------\n",
    "# General Use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For Saving Files\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* LOADING THE DATASET ***********************\n",
    "\n",
    "# Given the split dataset directory, return the train/test split\n",
    "def load_dataset(split_data_dir):\n",
    "    pickle_in = open(split_data_dir+'X_train.pickle','rb')\n",
    "    X_train = pickle.load(pickle_in)\n",
    "    \n",
    "    pickle_in = open(split_data_dir+'X_test.pickle','rb')\n",
    "    X_test = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(split_data_dir+'y_train.pickle','rb')\n",
    "    y_train = pickle.load(pickle_in)\n",
    "\n",
    "    pickle_in = open(split_data_dir+'y_test.pickle','rb')\n",
    "    y_test = pickle.load(pickle_in)\n",
    "    return X_train,X_test,y_train,y_test\n",
    "\n",
    "def load_tokenizer(tokenizer_dir):\n",
    "    pickle_in = open(tokenizer_dir,'rb')\n",
    "    t = pickle.load(pickle_in)\n",
    "    return t\n",
    "    \n",
    "split_data_dir = './split_data/'\n",
    "X_train,X_test,y_train,y_test = load_dataset(split_data_dir)\n",
    "\n",
    "tokenizer_dir = 'tokenizer.pickle'\n",
    "t = load_tokenizer(tokenizer_dir)\n",
    "\n",
    "# ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****************** PREPROCESSING FUNCTION *********************\n",
    "\n",
    "# Filters corrupted, unusable/unused data from the dataset\n",
    "def filter_dataset(df):\n",
    "    df = df.drop(columns=['id','tid1','tid2']) # drop id columns\n",
    "    df = df.drop(columns=['title1_zh','title2_zh']) # drop chinese columns\n",
    "\n",
    "    # Remove symbols\n",
    "    df['title1_en'] = df['title1_en'].str.replace('[^a-zA-Z0-9 ]','')\n",
    "    df['title2_en'] = df['title2_en'].str.replace('[^a-zA-Z0-9 ]','')\n",
    "\n",
    "    # Replace empty strings with NaN values\n",
    "    df['title1_en'].replace('', np.nan, inplace=True)\n",
    "    df['title2_en'].replace('', np.nan, inplace=True)\n",
    "    \n",
    "    # Remove rows with no label\n",
    "    labels = ['unrelated','agreed','disagreed']\n",
    "    df = df[df.label.isin(labels)]\n",
    "    \n",
    "    # Remove Unnamed columns\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Drop rows with null values\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# Returns a datset with an equal sampling of each label\n",
    "def equalize_dataset_labels(df, seed=1):\n",
    "    # Get minimum label count\n",
    "    n = df.label.value_counts().min()\n",
    "    \n",
    "    # Grabbing equal amounts of training data from each class\n",
    "    dfa = df[df['label']=='unrelated'].sample(n,random_state=seed)\n",
    "    dfb = df[df['label']=='agreed'].sample(n,random_state=seed)\n",
    "    dfc = df[df['label']=='disagreed'].sample(n,random_state=seed)\n",
    "    \n",
    "    # Recombine dataset and shuffle\n",
    "    df = pd.concat([dfa,dfb,dfc])\n",
    "    df = df.sample(frac=1,random_state=seed)\n",
    "    return df\n",
    "\n",
    "# Convert labels to integers for predictions\n",
    "def encode_labels(df):\n",
    "    # encoding the labels\n",
    "    labels = {'unrelated':0,'agreed':1,'disagreed':2}\n",
    "    df['label'].replace(labels,inplace=True)\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "# ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******************* TOKENIZING FUNCTIONS **********************\n",
    "\n",
    "# Create a word tokenizer given dataframe(s)\n",
    "def create_tokenizer(*data, num_words=None, lower=True, split=' ', oov_token=None, filename='tokenizer'):\n",
    "    # create the tokenizer\n",
    "    t = Tokenizer(num_words=num_words, lower=lower, split=split, oov_token=oov_token)\n",
    "    \n",
    "    # fit tokenizer\n",
    "    for df in data:\n",
    "        t.fit_on_texts(df['title1_en'])  \n",
    "        t.fit_on_texts(df['title2_en'])  \n",
    "    \n",
    "    # save for future use\n",
    "    pickle_out = open(filename+'.pickle', 'wb')\n",
    "    pickle.dump(t,pickle_out)\n",
    "    pickle_out.close()\n",
    "    return t\n",
    "\n",
    "# Tokenizes titles and encodes labels, trains a word tokenizer that is saved to a file\n",
    "def tokenize(t, df, maxlen=20):\n",
    "    # fit the tokenizer on the documents  \n",
    "    data1 = pad_sequences(sequences=t.texts_to_sequences(df['title1_en']), maxlen=maxlen)\n",
    "    data2 = pad_sequences(sequences=t.texts_to_sequences(df['title2_en']), maxlen=maxlen)\n",
    "    \n",
    "    # recombine\n",
    "    df = pd.DataFrame(np.concatenate((data1,data2),axis=1)).join(df['label'])\n",
    "    return df\n",
    "\n",
    "# ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nembeddings_index = dict()\\n\\nEMBED_SIZE = 100\\nLSTM_SIZE = 2\\n\\n# -------------- Compile Parameters --------------\\nactivation = 'softmax'\\noptimizer = 'RMSProp'\\nloss = 'sparse_categorical_crossentropy'\\nmetrics = ['accuracy']\\n\\nSENTENCE_SIZE = int(X_train.shape[1]/2)\\nvocab_size = len(t.word_index) + 1\\n\\n# create a weight matrix for words in training docs\\nembedding_matrix = np.zeros((vocab_size, EMBED_SIZE))\\nfor word, i in t.word_index.items():\\n    embedding_vector = embeddings_index.get(word)\\n    if embedding_vector is not None:\\n        embedding_matrix[i] = embedding_vector\\n\\n# FIRST MODEL: TITLE1_EN\\nfirst_input = Input(shape=(SENTENCE_SIZE,))\\nm1 = Embedding(vocab_size,\\n                EMBED_SIZE,\\n                weights=[embedding_matrix],\\n                input_length=SENTENCE_SIZE,\\n                trainable=False)(first_input)\\nm1 = Bidirectional(LSTM(LSTM_SIZE,dropout=0.2, recurrent_dropout=0.2))(m1)\\n\\n# SECOND MODEL: TITLE2_EN\\nsecond_input = Input(shape=(SENTENCE_SIZE,))\\nm2 = Embedding(vocab_size,\\n                 EMBED_SIZE,\\n                 weights=[embedding_matrix],\\n                 input_length=SENTENCE_SIZE,\\n                 trainable=False)(second_input)\\nm2 = Bidirectional(LSTM(LSTM_SIZE,dropout=0.2, recurrent_dropout=0.2))(m2)\\n\\n# MERGE MODEL\\nmerged = Concatenate(axis=1)([m1, m2])\\noutput_layer = Dense(3, activation='softmax')(merged)\\n\\nmodel = Model(inputs=[first_input, second_input], outputs=output_layer)\\nmodel.compile(optimizer=optimizer, loss=loss,metrics=metrics)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "embeddings_index = dict()\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "LSTM_SIZE = 2\n",
    "\n",
    "# -------------- Compile Parameters --------------\n",
    "activation = 'softmax'\n",
    "optimizer = 'RMSProp'\n",
    "loss = 'sparse_categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "SENTENCE_SIZE = int(X_train.shape[1]/2)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, EMBED_SIZE))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# FIRST MODEL: TITLE1_EN\n",
    "first_input = Input(shape=(SENTENCE_SIZE,))\n",
    "m1 = Embedding(vocab_size,\n",
    "                EMBED_SIZE,\n",
    "                weights=[embedding_matrix],\n",
    "                input_length=SENTENCE_SIZE,\n",
    "                trainable=False)(first_input)\n",
    "m1 = Bidirectional(LSTM(LSTM_SIZE,dropout=0.2, recurrent_dropout=0.2))(m1)\n",
    "\n",
    "# SECOND MODEL: TITLE2_EN\n",
    "second_input = Input(shape=(SENTENCE_SIZE,))\n",
    "m2 = Embedding(vocab_size,\n",
    "                 EMBED_SIZE,\n",
    "                 weights=[embedding_matrix],\n",
    "                 input_length=SENTENCE_SIZE,\n",
    "                 trainable=False)(second_input)\n",
    "m2 = Bidirectional(LSTM(LSTM_SIZE,dropout=0.2, recurrent_dropout=0.2))(m2)\n",
    "\n",
    "# MERGE MODEL\n",
    "merged = Concatenate(axis=1)([m1, m2])\n",
    "output_layer = Dense(3, activation='softmax')(merged)\n",
    "\n",
    "model = Model(inputs=[first_input, second_input], outputs=output_layer)\n",
    "model.compile(optimizer=optimizer, loss=loss,metrics=metrics)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 47254: expected 7 fields, saw 9\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id\n",
      "0       41937\n",
      "1      117063\n",
      "2      306750\n",
      "3        5069\n",
      "4      218992\n",
      "5       11569\n",
      "6      319706\n",
      "7      106194\n",
      "8      212572\n",
      "9       25238\n",
      "10     197559\n",
      "11     150201\n",
      "12     308268\n",
      "13     184546\n",
      "14     189459\n",
      "15     222176\n",
      "16      59898\n",
      "17     182649\n",
      "18      37649\n",
      "19      80201\n",
      "20     198449\n",
      "21     232173\n",
      "22      84431\n",
      "23      32877\n",
      "24     179366\n",
      "25     171745\n",
      "26       4352\n",
      "27     238881\n",
      "28      93581\n",
      "29     295716\n",
      "...       ...\n",
      "64112  302833\n",
      "64113  271435\n",
      "64114  166415\n",
      "64115   31939\n",
      "64116  139174\n",
      "64117  102320\n",
      "64118   19553\n",
      "64119   34531\n",
      "64120  204822\n",
      "64121  194357\n",
      "64122   93440\n",
      "64123  229085\n",
      "64124  318419\n",
      "64125   61636\n",
      "64126  193994\n",
      "64127   81989\n",
      "64128   23561\n",
      "64129  161241\n",
      "64130  213605\n",
      "64131   58763\n",
      "64132   31935\n",
      "64133  315316\n",
      "64134   65843\n",
      "64135   40923\n",
      "64136   59521\n",
      "64137  214559\n",
      "64138  229127\n",
      "64139  277207\n",
      "64140   33059\n",
      "64141  144818\n",
      "\n",
      "[64142 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# preprocess the csv and tokenize it\\nnew_t = create_tokenizer(dataFrame_test, num_words=40000, oov_token=None)\\ndf_test_encoded = encode_labels(dataFrame_test)\\ndf_test_tokenized = tokenize(t,df_test_encoded,25)\\n\\n# create a new csv (just writing to one i created)\\n#prediction = model.predict(...)\\nprediction = pd.DataFrame(prediction)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ************************ MAIN *********************************\n",
    "import pandas as pd\n",
    "\n",
    "input_test = './data/test.csv'\n",
    "\n",
    "# load test.csv into a dataframe and preprocess it\n",
    "df_test = pd.read_csv(input_test,encoding='utf-8-sig',error_bad_lines=False)\n",
    "dataFrame_test = pd.DataFrame(df_test)\n",
    "dataFrame_test = dataFrame_test.drop(columns=['tid1', 'tid2', 'title1_en', 'title1_zh', 'title2_zh', 'title2_en']) # drop columns\n",
    "\n",
    "# add predictions\n",
    "\"\"\"\n",
    "split_data_dir = './split_data/'\n",
    "X_train,X_test,y_train,y_test = load_dataset(split_data_dir)\n",
    "\n",
    "index = 100\n",
    "\n",
    "first_sentence = X_train[index][:SENTENCE_SIZE].reshape(1,SENTENCE_SIZE)\n",
    "second_sentence = X_train[index][SENTENCE_SIZE:SENTENCE_SIZE*2].reshape(1,SENTENCE_SIZE)\n",
    "\n",
    "prediction = model.predict([first_sentence,second_sentence])\n",
    "\n",
    "prediction_list = [int(round(p)) for p in prediction[0]]\n",
    "print(prediction_list)\n",
    "\"\"\"\n",
    "\n",
    "# write dataframe to csv\n",
    "print(dataFrame_test)\n",
    "dataFrame_test.insert(1, 'label', 0) #replace 0 with prediction data\n",
    "dataFrame_test.to_csv('./data/submission.csv', index = False)\n",
    "\n",
    "\"\"\"\n",
    "# preprocess the csv and tokenize it\n",
    "new_t = create_tokenizer(dataFrame_test, num_words=40000, oov_token=None)\n",
    "df_test_encoded = encode_labels(dataFrame_test)\n",
    "df_test_tokenized = tokenize(t,df_test_encoded,25)\n",
    "\n",
    "# create a new csv (just writing to one i created)\n",
    "#prediction = model.predict(...)\n",
    "prediction = pd.DataFrame(prediction)\n",
    "\"\"\"\n",
    "\n",
    "# ***************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
